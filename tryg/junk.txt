
    with tf.variable_scope("resets"):
        with tf.variable_scope('gmmreset_layer'):
            
            killers=[]
            
            # posterior on categorical
            killers.append(thevars_dict['posterior_categ/hidden2categ_layer/kernel'].initializer)
            killers.append(thevars_dict['posterior_categ/hidden2categ_layer/bias'].initializer)
            killers.append(thevars_dict['posterior_categ/mean2categ_layer/kernel'].initializer)
            killers.append(thevars_dict['posterior_categ/mean2categ_layer/bias'].initializer)
            
            # prior on gaussians
            gaustdsz=tf.placeholder(DD,name='gaustdsz')
            inits=tf.random_normal([Nclust,Nk],name='W',dtype=DD)
            mn = GR.get_tensor_by_name('posterior_samples/Z_aggregate_mean:0') # 1 x Nk
            stdscalar = GR.get_tensor_by_name('posterior_samples/Z_aggregate_std:0') # scalar
            # broadcast mn to Nclust x Nk
            mn=tf.tile(mn,[Nclust,1])
            # broadcast std to Nclust
            std=tf.reshape(stdscalar,[1])
            std=tf.tile(std,[Nclust])
            # do the kill
            killers.append(tf.assign(thevars_dict['prior/mean'],inits*gaustdsz*stdscalar*.1+mn,name='killpriormean'))
            killers.append(tf.assign(thevars_dict['prior/std_almost'],std*gaustdsz,name='killpriorstd'))
            
            # prior on categorical
            killers.append(thevars_dict['prior/categ_almost'].initializer)
            
        gmmreset=tf.group(*killers,name='gmmreset')

def init(check=False,multi=None,threads=1):
    with GR.as_default():
        if 'sess' in locals():
            if isinstance(sess,list):
                for s in sess:
                    s.close()
            else:
                sess.close()
        if 'sess' in globals():
            sess=globals()['sess']
            if isinstance(sess,list):
                for s in sess:
                    s.close()
            else:
                sess.close()
            
        if multi is None:
            sess=tf.Session()
            sess.run(globinit)
            
        else:
            sess=[]
            for i in range(multi):
                if threads is None:
                    sess.append(tf.Session())
                else:
                    session_conf = tf.ConfigProto(
                      intra_op_parallelism_threads=threads,
                      inter_op_parallelism_threads=threads,
                      device_count={'CPU': threads})
                    
                    print(session_conf)
                    sess.append(tf.Session(config=session_conf))
                sess[-1].run(globinit)
    return sess
# sess=init(True)
sess=init(True,multi=2)

# train for a bit using only the first layer of hierarchy
with GR.as_default():
    
    binsize=1000
    dct = make_lr_dict(data=allguys[:binsize],node_mask='primeonly',
                       encoder=.00001,decoder=.00001,prior=.00001,encoder_pi=.00001,alpha=.1) 
    
#     nsweep=50000
    nsweep=100000
    bins=np.r_[0:allguys.shape[0]:binsize]

    assigs=['loss_computation/final:0']
    assigs.append('optimizers/all')
#     assigs.append('batchnorms/all')
    
    errs=[]
    for i in bat.range_noisy(nsweep):
        
        # batch normalize
        
        
        # run an epoch
        for j in range(len(bins)-1):
            dct[data]=allguys[bins[j]:bins[j+1]]
            loss=sess.run(assigs,feed_dict=dct)[0]
            errs.append(loss)
